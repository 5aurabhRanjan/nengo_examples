{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Existing Networks\n",
    "\n",
    "Nengo DL is not confined to opimizing cutom made networks, it can also be used to make existing networks better, or acheive the same result with less neurons. What this example will show is how to train a circular convolution network.\n",
    "\n",
    "Circular convolution is a key opeartion used to process [semantic pointers](http://compneuro.uwaterloo.ca/research/spa/semantic-pointer-architecture.html) and by optimizing this smaller network, larger more complex networks that utilize circular convolution can benefit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nengo\n",
    "import nengo_dl\n",
    "%load_ext nengo.ipynb\n",
    "from nengo.spa import Vocabulary\n",
    "\n",
    "import random\n",
    "import string\n",
    "import sys\n",
    "from io import StringIO\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To properly train the network, we generate novel training data by randomly generating semantic pointers. Otherwise, by simply training on the same pointers each time the network will quickly overfit the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_pointers(amount, vocabulary):\n",
    "    pairs = []\n",
    "    for v in range(amount):\n",
    "        # generate amount pairs\n",
    "        # keys start with A, second element starts with B, third starts with C\n",
    "        conv_key = 'C' + str(v)\n",
    "        point_key_1= 'A' + str(v)\n",
    "        pointer_1 = vocabulary.create_pointer(attempts=500)\n",
    "        point_key_2 = 'B' + str(v)\n",
    "        pointer_2 = vocabulary.create_pointer(attempts=500)\n",
    "        vocabulary.add(point_key_1, pointer_1)\n",
    "        vocabulary.add(point_key_2, pointer_2)\n",
    "        vocabulary.add(conv_key, vocabulary.parse(point_key_2 + \"*\" + point_key_1))\n",
    "        \n",
    "def repeat_pointer(pointer, steps):\n",
    "    return np.repeat(np.expand_dims(pointer, axis=0), steps, axis=0)\n",
    "\n",
    "def generate_data(dims, seed, amount, steps):\n",
    "    state = np.random.RandomState(seed)\n",
    "    vocabulary = Vocabulary(dimensions=dims, rng=state)\n",
    "    gen_pointers(amount, vocabulary)\n",
    "    A = np.asarray([repeat_pointer(vocabulary['A' + str(i)].v,steps) for i in range(amount)])\n",
    "    B = np.asarray([repeat_pointer(vocabulary['B' + str(i)].v,n_steps) for i in range(amount)])\n",
    "    C = np.asarray([repeat_pointer(vocabulary['C' + str(i)].v,n_steps) for i in range(amount)])\n",
    "    return A, B, C, vocabulary\n",
    "\n",
    "# Number of dimensions for the Semantic Pointers\n",
    "dimensions = 50\n",
    "n_steps = 300\n",
    "minibatch_size = 10\n",
    "\n",
    "test_a, test_b, test_c, vocab = generate_data(dimensions, 0, minibatch_size, n_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To model the network in a way which will yield useful final results spiking neurons are used. This way, once we optimize the convolution portion of the network it is still able to be used in other models.\n",
    "\n",
    "The network is not very complex, the circular convolution portion is already defined by Nengo, all that is required is connecting input nodes to the circular convolution network which will feed in the pointers to be convoluted.\n",
    "\n",
    "In this example only 5 neurons are used per dimension for the circular convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = nengo.Network()\n",
    "with model:\n",
    "    model.config[nengo.Ensemble].neuron_type = nengo_dl.SoftLIFRate(sigma=0.1, tau_rc=0.022, tau_ref=0.002)\n",
    "    model.config[nengo.Ensemble].gain = nengo.dists.Choice([1])\n",
    "    model.config[nengo.Ensemble].bias = nengo.dists.Uniform(-1, 1)\n",
    "\n",
    "    # Get the raw vectors for the pointers using `vocab['A'].v`\n",
    "    a = nengo.Node(output=vocab['A0'].v)\n",
    "    b = nengo.Node(output=vocab['B0'].v)\n",
    "\n",
    "    # Make the circular convolution network with 5 neurons per dimension\n",
    "    cconv = nengo.networks.CircularConvolution(5, dimensions=dimensions)\n",
    "\n",
    "    # Connect the input nodes to the input slots `A` and `B` on the network\n",
    "    nengo.Connection(a, cconv.input_a)\n",
    "    nengo.Connection(b, cconv.input_b)\n",
    "\n",
    "    # Probe the output\n",
    "    out = nengo.Probe(cconv.output, synapse=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run the network in its default state to get an idea of the baseline performance. The performance shown is very poor, there is not much of a distinction between the output of the network and the rest of the pointers in the vocabulary. Ideally the output would be clearly `C0`, the result of the convolution between `A0` and `B0` which are the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with nengo.Simulator(model) as sim:\n",
    "    sim.run(0.3)\n",
    "plt.figure()\n",
    "plt.plot(sim.trange(), nengo.spa.similarity(sim.data[out], vocab))\n",
    "plt.legend(vocab.keys, loc=4)\n",
    "plt.xlabel(\"t [s]\")\n",
    "plt.ylabel(\"dot product\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the network the standard procedure can be followed, however in this case the standard objective function will not used. Instead the cosine distance between the target pointer (the true result of the convolution) and the network output will be calculated. Throughout training the objective will be to reduce that distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def objective_cosine(outputs, targets):\n",
    "    return tf.abs(tf.losses.cosine_distance(targets, outputs, dim=2))\n",
    "    \n",
    "with nengo_dl.Simulator(model, step_blocks=n_steps, minibatch_size=minibatch_size, device=\"/cpu:0\") as sim:\n",
    "    test_input_feed = {a:test_a,\n",
    "                       b:test_b}\n",
    "    test_output_feed = {out: test_c}    \n",
    "    optimizer = tf.train.RMSPropOptimizer(5e-3, decay=0.9, momentum=0.0, epsilon=1e-10, use_locking=False, centered=False, name='RMSProp')\n",
    "    losses = []\n",
    "    print(\"LOSS: \" + str(sim.loss(test_input_feed, test_output_feed, objective_cosine)))\n",
    "    for e in range(120):\n",
    "        print(\"ROUND: \" + str(e))\n",
    "        train_a, train_b, train_c, _ = generate_data(dimensions, e+1, minibatch_size, n_steps)\n",
    "        input_feed = {a:train_a, \n",
    "                      b: train_b}\n",
    "        output_feed = {out: train_c}\n",
    "        sim.train(input_feed, output_feed, optimizer, n_epochs=1, objective=objective_cosine)\n",
    "        loss = sim.loss(test_input_feed, test_output_feed, objective_cosine)\n",
    "        losses.append(loss)\n",
    "        print(\"LOSS: \" + str(loss))\n",
    "    sim.save_params('circular_convolution-softlif')\n",
    "    sim.run(0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the progress of the network we plot the loss vs. the training round. Near the end of the trainining we show that the network has hit its minimum, as instead of continuing to decrease as the trend would suggest it \"bottoms out\" and starts fluctuating around the minimum value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Training Round')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.suptitle('Test Loss vs. Training Round')\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After training we run the same test on the network and plot the output. The improvement is very clear, prior to training the network's values were all very low, and some too close together to give a useful output. Now it is clearly shown the output vector is closest to `C0`, which is the numerical result of the circular convolution of `A0` and `B0` which were the two inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = sim.data[out]\n",
    "plt.figure()\n",
    "plt.plot(sim.trange(), nengo.spa.similarity(output[0], vocab))\n",
    "plt.legend(vocab.keys, loc=4)\n",
    "plt.xlabel(\"t [s]\")\n",
    "plt.ylabel(\"dot product\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a future example we will show how to integrate these training improvments into a larger network and improve the performance of the network as a whole"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
